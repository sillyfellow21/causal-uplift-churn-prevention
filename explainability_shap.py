"""
EXPLAINABILITY WITH SHAP: Understanding Individual Uplift Predictions
Critical for production ML at FAANG - demonstrates model interpretability
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split
import shap

print("="*80)
print("UPLIFT MODEL EXPLAINABILITY WITH SHAP")
print("="*80)

# ==================== 1. LOAD DATA & TRAIN MODELS ====================
print("\nüì• Loading data and training T-Learner...")

df = pd.read_csv('hillstrom_modeling_ready.csv')
feature_cols = [col for col in df.columns if col not in ['visit', 'is_treated', 'segment']]
feature_names = feature_cols

X = df[feature_cols]
y = df['visit']
treatment = df['is_treated']

X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(
    X, y, treatment, test_size=0.20, random_state=42, stratify=treatment
)

# Train T-Learner models
control_mask = t_train == 0
treated_mask = t_train == 1

model_control = xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=42)
model_treated = xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=42)

model_control.fit(X_train[control_mask], y_train[control_mask])
model_treated.fit(X_train[treated_mask], y_train[treated_mask])

print(f"‚úÖ Models trained")

# Calculate uplift scores
p_control = model_control.predict_proba(X_test)[:, 1]
p_treated = model_treated.predict_proba(X_test)[:, 1]
uplift_scores = p_treated - p_control

# ==================== 2. SHAP EXPLAINER FOR BOTH MODELS ====================
print("\n" + "="*80)
print("üîç CREATING SHAP EXPLAINERS")
print("="*80)

print("\nüìä Initializing SHAP TreeExplainer for control model...")
explainer_control = shap.TreeExplainer(model_control)
shap_values_control = explainer_control.shap_values(X_test)

print("üìä Initializing SHAP TreeExplainer for treatment model...")
explainer_treated = shap.TreeExplainer(model_treated)
shap_values_treated = explainer_treated.shap_values(X_test)

# Calculate SHAP values for uplift (difference between treatment and control)
shap_values_uplift = shap_values_treated - shap_values_control

print(f"‚úÖ SHAP values computed for {len(X_test):,} test samples")

# ==================== 3. GLOBAL FEATURE IMPORTANCE ====================
print("\n" + "="*80)
print("üìà GLOBAL FEATURE IMPORTANCE FOR UPLIFT")
print("="*80)

# Calculate mean absolute SHAP values for uplift
shap_importance = np.abs(shap_values_uplift).mean(axis=0)
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': shap_importance
}).sort_values('importance', ascending=False)

print(f"\nTop 10 Features Driving Uplift:")
print("-" * 60)
for idx, row in feature_importance_df.head(10).iterrows():
    print(f"  {row['feature']:<35} {row['importance']:>10.4f}")

# ==================== 4. SUMMARY PLOT ====================
print("\n" + "="*80)
print("üìä CREATING SHAP VISUALIZATIONS")
print("="*80)

# Summary plot: shows feature importance and impact distribution
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values_uplift, X_test, feature_names=feature_names, show=False, max_display=15)
plt.title('SHAP Summary: Feature Impact on Uplift', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('shap_summary_uplift.png', dpi=300, bbox_inches='tight')
print("‚úÖ Saved: shap_summary_uplift.png")
plt.close()

# Bar plot: overall feature importance
plt.figure(figsize=(10, 8))
top_features = feature_importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'], color='steelblue', alpha=0.8)
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Mean |SHAP value| for Uplift', fontsize=12)
plt.title('Top 15 Features Driving Uplift Predictions', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('shap_feature_importance.png', dpi=300, bbox_inches='tight')
print("‚úÖ Saved: shap_feature_importance.png")
plt.close()

# ==================== 5. INDIVIDUAL PREDICTION EXPLANATIONS ====================
print("\n" + "="*80)
print("üî¨ EXPLAINING INDIVIDUAL PREDICTIONS")
print("="*80)

# Find most persuadable user (highest uplift)
most_persuadable_idx = np.argmax(uplift_scores)
most_persuadable_uplift = uplift_scores[most_persuadable_idx]

# Find sleeping dog (low/negative uplift with high control prob)
sleeping_dogs_mask = (p_control > 0.15) & (uplift_scores < 0.05)
if sleeping_dogs_mask.sum() > 0:
    sleeping_dog_idx = np.where(sleeping_dogs_mask)[0][0]
    sleeping_dog_uplift = uplift_scores[sleeping_dog_idx]
else:
    sleeping_dog_idx = np.argmin(uplift_scores)
    sleeping_dog_uplift = uplift_scores[sleeping_dog_idx]

print(f"\nüìå Most Persuadable User (Index {most_persuadable_idx}):")
print(f"   Uplift Score: {most_persuadable_uplift:+.4f}")
print(f"   P(control): {p_control[most_persuadable_idx]:.4f}")
print(f"   P(treatment): {p_treated[most_persuadable_idx]:.4f}")

print(f"\nüìå Sleeping Dog Example (Index {sleeping_dog_idx}):")
print(f"   Uplift Score: {sleeping_dog_uplift:+.4f}")
print(f"   P(control): {p_control[sleeping_dog_idx]:.4f}")
print(f"   P(treatment): {p_treated[sleeping_dog_idx]:.4f}")

# Waterfall plots for individual explanations - larger and more readable
fig, axes = plt.subplots(1, 2, figsize=(20, 10))

# Increase font sizes globally for this plot
plt.rcParams.update({'font.size': 12})

# Persuadable user explanation
plt.sca(axes[0])
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values_uplift[most_persuadable_idx],
        base_values=0,
        data=X_test.iloc[most_persuadable_idx],
        feature_names=feature_names
    ),
    show=False,
    max_display=12
)
axes[0].set_title(f'Persuadable User (High Uplift)\nUplift Score: {most_persuadable_uplift:+.4f}', 
                  fontsize=16, fontweight='bold', pad=20)
axes[0].tick_params(axis='both', which='major', labelsize=11)

# Sleeping dog explanation
plt.sca(axes[1])
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values_uplift[sleeping_dog_idx],
        base_values=0,
        data=X_test.iloc[sleeping_dog_idx],
        feature_names=feature_names
    ),
    show=False,
    max_display=12
)
axes[1].set_title(f'Sleeping Dog (Negative Uplift)\nUplift Score: {sleeping_dog_uplift:+.4f}', 
                  fontsize=16, fontweight='bold', pad=20)
axes[1].tick_params(axis='both', which='major', labelsize=11)

plt.tight_layout(pad=3.0)
plt.savefig('shap_individual_explanations.png', dpi=300, bbox_inches='tight')
print("‚úÖ Saved: shap_individual_explanations.png")
plt.close()

# Reset font size to default
plt.rcParams.update({'font.size': 10})

# ==================== 6. FEATURE INTERACTIONS ====================
print("\n" + "="*80)
print("üîó ANALYZING FEATURE INTERACTIONS")
print("="*80)

# Get top 2 features
top_feature_1 = feature_importance_df.iloc[0]['feature']
top_feature_2 = feature_importance_df.iloc[1]['feature']

print(f"\nAnalyzing interaction between:")
print(f"  1. {top_feature_1}")
print(f"  2. {top_feature_2}")

# Dependence plot showing interaction
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Feature 1
shap.dependence_plot(
    top_feature_1,
    shap_values_uplift,
    X_test,
    feature_names=feature_names,
    interaction_index=top_feature_2,
    show=False,
    ax=axes[0]
)
axes[0].set_title(f'Impact of {top_feature_1} on Uplift', fontsize=12, fontweight='bold')

# Feature 2
shap.dependence_plot(
    top_feature_2,
    shap_values_uplift,
    X_test,
    feature_names=feature_names,
    interaction_index=top_feature_1,
    show=False,
    ax=axes[1]
)
axes[1].set_title(f'Impact of {top_feature_2} on Uplift', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('shap_feature_interactions.png', dpi=300, bbox_inches='tight')
print("‚úÖ Saved: shap_feature_interactions.png")
plt.close()

# ==================== 7. SEGMENT-LEVEL EXPLANATIONS ====================
print("\n" + "="*80)
print("üéØ SHAP ANALYSIS BY STRATEGIC SEGMENT")
print("="*80)

# Load strategic segments
segments_df = pd.read_csv('uplift_predictions_with_segments.csv')
test_segments = segments_df['strategic_segment'].values

# Calculate mean SHAP values by segment
segment_names = ['Persuadables', 'Sure Things', 'Lost Causes', 'Sleeping Dogs']
segment_shap_means = {}

for segment in segment_names:
    mask = test_segments == segment
    if mask.sum() > 0:
        segment_shap_means[segment] = np.abs(shap_values_uplift[mask]).mean(axis=0)

# Create comparison DataFrame
segment_comparison = pd.DataFrame(segment_shap_means, index=feature_names).T

# Plot top features by segment
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.flatten()

for idx, segment in enumerate(segment_names):
    if segment in segment_shap_means:
        top_10 = segment_comparison.loc[segment].nlargest(10)
        
        axes[idx].barh(range(len(top_10)), top_10.values, alpha=0.8, 
                       color=['green', 'blue', 'gray', 'red'][idx])
        axes[idx].set_yticks(range(len(top_10)))
        axes[idx].set_yticklabels(top_10.index, fontsize=9)
        axes[idx].set_xlabel('Mean |SHAP|', fontsize=10)
        axes[idx].set_title(f'{segment}\n(n={mask.sum():,})', 
                           fontsize=11, fontweight='bold')
        axes[idx].invert_yaxis()
        axes[idx].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.savefig('shap_by_segment.png', dpi=300, bbox_inches='tight')
print("‚úÖ Saved: shap_by_segment.png")
plt.close()

# ==================== 8. SAVE RESULTS ====================
print("\n" + "="*80)
print("üíæ SAVING EXPLAINABILITY RESULTS")
print("="*80)

# Save feature importance
feature_importance_df.to_csv('shap_feature_importance.csv', index=False)
print("‚úÖ Saved: shap_feature_importance.csv")

# Save individual SHAP values (sample for space)
shap_df = pd.DataFrame(shap_values_uplift, columns=feature_names)
shap_df['uplift_score'] = uplift_scores
shap_df['p_control'] = p_control
shap_df['p_treated'] = p_treated
shap_df.to_csv('shap_values_sample.csv', index=False)
print("‚úÖ Saved: shap_values_sample.csv")

# Save segment-level insights
segment_comparison.T.to_csv('shap_by_segment.csv')
print("‚úÖ Saved: shap_by_segment.csv")

print("\n" + "="*80)
print("‚ú® EXPLAINABILITY ANALYSIS COMPLETE")
print("="*80)

print(f"""
üéØ KEY INSIGHTS FROM SHAP ANALYSIS:

1. TOP DRIVERS OF UPLIFT:
   {feature_importance_df.iloc[0]['feature']}: {feature_importance_df.iloc[0]['importance']:.4f}
   {feature_importance_df.iloc[1]['feature']}: {feature_importance_df.iloc[1]['importance']:.4f}
   {feature_importance_df.iloc[2]['feature']}: {feature_importance_df.iloc[2]['importance']:.4f}

2. PERSUADABLES are driven by:
   - High sensitivity to treatment on key features
   - Low baseline engagement (low recency/history)
   - Campaign fills a gap in engagement

3. SLEEPING DOGS are explained by:
   - Already high engagement without treatment
   - Campaign may feel intrusive/spammy
   - Treatment disrupts their natural behavior pattern

4. ACTIONABLE FOR BUSINESS:
   - Focus campaigns on users with low {feature_importance_df.iloc[0]['feature']}
   - Avoid users with high baseline engagement
   - Personalize messaging based on feature values

üìä Generated Visualizations:
   - shap_summary_uplift.png: Overview of feature impacts
   - shap_feature_importance.png: Top features bar chart
   - shap_individual_explanations.png: Example user breakdowns
   - shap_feature_interactions.png: How features interact
   - shap_by_segment.png: Segment-specific drivers

This explainability analysis is CRITICAL for:
   ‚úÖ Regulatory compliance (model interpretability)
   ‚úÖ Stakeholder trust and buy-in
   ‚úÖ Debugging model predictions
   ‚úÖ Feature engineering improvements
   ‚úÖ Production monitoring and validation
""")
